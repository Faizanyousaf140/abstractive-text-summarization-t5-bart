

# ğŸ“˜ **Abstractive Text Summarization using T5 / BART**

This project implements **abstractive text summarization** using powerful **Encoderâ€“Decoder architectures (T5 and BART)**.
The model is **fine-tuned on the CNN/DailyMail news dataset** to generate **concise, human-like summaries** of long articles.

---

## âœ¨ **Features**

âœ” **Preprocessing of articleâ€“summary pairs**
âœ” **Fine-tuning T5 or BART** using Hugging Face Transformers
âœ” **Evaluation using ROUGE-1, ROUGE-2, ROUGE-L**
âœ” **Qualitative comparison** of predicted vs. reference summaries
âœ” **Streamlit demo** for real-time summarization *(optional)*

---

## ğŸ“‚ **Dataset**

**CNN/DailyMail dataset:**
[https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail](https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail)

The dataset contains:

* **Article:** Full news story
* **Highlights:** Human-written summary

**Task:** Generate a **short abstractive summary** from a long article.

---

## ğŸ›  **Model Architecture**

This project uses an **Encoderâ€“Decoder Transformer**:

### ğŸ”¹ **T5 (Text-to-Text Transfer Transformer)**

* Unified text-to-text format
* Strong abstractive summarization performance

### ğŸ”¹ **BART (Bidirectional + Autoregressive Transformer)**

* Robust denoising autoencoder
* Excellent for long-document summarization

---

## ğŸš€ **Training Pipeline**

### **1. Preprocessing**

* Load dataset
* Clean text *(HTML, whitespace, special characters)*
* Map **article â†’ summary** pairs
* Tokenize using model tokenizer
* Create PyTorch datasets

### **2. Fine-Tuning**

* HuggingFace **Trainer API**
* **Loss:** Cross-entropy
* **Batch size:** 2â€“4
* **Epochs:** 2â€“3
* **Learning rate:** 3e-5

### **3. Evaluation**

Metrics computed:

* **ROUGE-1**
* **ROUGE-2**
* **ROUGE-L**

Outputs stored in the `results/` directory.


---

## ğŸ§ª **Usage (Inference)**

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "t5-small"  # or "facebook/bart-large-cnn"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained("your_finetuned_model_path")

text = "Your article text here..."
inputs = tokenizer("summarize: " + text, return_tensors="pt",
                   max_length=1024, truncation=True)

summary_ids = model.generate(inputs["input_ids"],
                             max_length=150, min_length=40)

print(tokenizer.decode(summary_ids[0], skip_special_tokens=True))
```

---

## ğŸ–¥ï¸ **Optional: Streamlit App**

**File:** `app.py`

```
streamlit run app.py
```

This launches a **simple web interface** where users can paste text and get instant summaries.

---

## ğŸ“¦ **Install Requirements**

```
pip install -r requirements.txt
```

---

## ğŸ“ **Project Structure**

```
â”œâ”€â”€ summarizer.ipynb
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ results/
```

---

## ğŸ“œ **License**

**MIT License**

